name: latest
on:
  push:
  workflow_dispatch:

jobs:
  secret_scan:
    name: Secret Scan
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v2

      - name: Set up Python
        uses: actions/setup-python@v2
        with:
          python-version: 3.7

      - name: Install dependencies
        run: |
          sudo apt-get install -y diffutils jq python3 python3-pip
          python -m pip install detect-secrets
          python3 -m pip install tabulate
          python -m pip install --upgrade pip
          
      - name: Run Detect Secrets Scan
        run: |
          detect-secrets scan --exclude-files '\.git/.*' --exclude-files '\.gitmodules' >> detect_secrets_output.txt
      

      - name: Parse Detect Secrets Output
        id: parse_output
        run: |
          import json
          import os  # Import the os module
          import sys
          import datetime 
          from tabulate import tabulate
          with open("detect_secrets_output.txt", "r") as file:
              data = file.read()
          data_json = json.loads(data)
          if not data_json:
              sys.exit(0)
              
          table = []  # Initialize an empty table
          table_headers = ["Secret Type", "File", "Hashed Secret", "Line Number"]
          for file, secrets in data_json["results"].items():
              for secret in secrets:
                  if not isinstance(secret, dict):
                    continue
                  secret_type = secret.get("type", "NA")
                  filename = secret.get("filename", "NA")
                  hashed_secret = secret.get("hashed_secret", "NA")
                  line_number = secret.get("line_number", "NA")
                  row = [secret_type, filename, hashed_secret, line_number]
                  table.append(row)
                     
          print(tabulate(table, headers=table_headers, tablefmt='grid'))
          # Naming convention for the txt file
          nameofuser = "cairo"  # Replace this with your username
          nameofjob = "detect_secret"   # Replace this with the job name or any relevant identifier
          # Get the current date and time
          now = datetime.datetime.now()
          date_and_time = now.strftime("%Y-%m-%d_%H-%M")
          filename = f"{nameofuser}_{nameofjob}_{date_and_time}.txt"
          file_path = os.path.join(os.path.dirname(os.path.abspath(__file__)), filename)
          with open(file_path, "w") as file:
              file.write(tabulate(table, headers=table_headers, tablefmt='grid'))
          print(f'::set-output name=file-path::{file_path}')
        shell: python
        
      - name: Authenticate to Google Cloud
        id: auth
        uses: google-github-actions/auth@v1
        with:
          credentials_json: '${{ secrets.GCS_SA_KEY }}'

      - name: Upload Detect Secrets Output to GCS
        uses: google-github-actions/upload-cloud-storage@v1
        with:
          path: ${{ steps.parse_output.outputs.file-path }}
          destination: 'cicdpipelinee/Detect secret'
          
      - name: Check for Secrets and Exit
        run: |
          table_file="${{ steps.parse_output.outputs.file-path }}"
          table_length=$(wc -l < "$table_file")
          if [ "$table_length" -gt 0 ]; then
            echo "Secrets found in the table. Exiting workflow..."
            exit 1
          else
            echo "No secrets found. Continuing with the workflow."
          fi
          
  Trufflehog:

    name: Trufflehog Scan
    #needs: secret_scan
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v2

      - name: Run truffleHog
        run: |
          python3 -m venv env
          source env/bin/activate
          python -m pip install truffleHog
          python env/bin/trufflehog  --regex --entropy=False  --json https://github.com/yashwarrhan/pepper.git > truffleHog.json || true
      
      - name: Install tabulate
        run: |
          python3 -m pip install tabulate
      
      - name: Parse trufflehog JSON results
        id: parse_trufflehog

        run: |  
          import json
          import os
          import datetime
          import textwrap  # Import the textwrap module
          from tabulate import tabulate

          def parse_trufflehog_output(file_path):
              table = []  # Initialize an empty table
              table_headers = ["Branch", "Commit", "Commit Hash", "Date", "Reason"]

              with open(file_path, "r") as file:
                  for line in file:
                      try:
                          data = json.loads(line)
                          row = [data["branch"], data["commit"], data["commitHash"], data["date"], data["reason"]]
                          table.append(row)
                      except json.JSONDecodeError as e:
                          print(f"Failed to parse line as JSON: {line}")

              return table_headers, table

          def wrap_text_in_table(table_data, table_headers, width=20):
              wrapped_table = [table_headers]  # Add headers to the wrapped table
              for row in table_data:
                  wrapped_row = [textwrap.fill(str(item), width) if header != "Commit Hash" else str(item) for header, item in zip(table_headers, row)]
                  wrapped_table.append(wrapped_row)
              return wrapped_table

          if __name__ == "__main__":
              input_file_path = "truffleHog.json"  # Use the correct path to the truffleHog.json file here
              table_headers, table_data = parse_trufflehog_output(input_file_path)

              # Wrap the text in the table for better display
              wrapped_table_data = wrap_text_in_table(table_data, table_headers)

              # Print the wrapped table to the console
              print(tabulate(wrapped_table_data, headers=table_headers, tablefmt='grid'))

              # Save the parsed data into a text file
              nameofuser = "yash"  # Replace this with your username
              nameofjob = "trufflehog"   # Replace this with the job name or any relevant identifier
              now = datetime.datetime.now()
              date_and_time = now.strftime("%Y-%m-%d_%H-%M")
              output_file_path = f"{nameofuser}_{nameofjob}_{date_and_time}.txt"

              # Write the wrapped table to the output file
              with open(output_file_path, "w") as file:
                  file.write(tabulate(wrapped_table_data, headers=table_headers, tablefmt='grid'))

              print(f'::set-output name=file-path::{output_file_path}')

          shell: python

      - name: Authenticate to Google Cloud
        id: auth_truffle
        uses: google-github-actions/auth@v1
        with:
          credentials_json: '${{ secrets.GCS_SA_KEY }}'

      - name: Upload Trufflehog Output to GCS
        uses: google-github-actions/upload-cloud-storage@v1
        with:
          path: ${{ steps.parse_trufflehog.outputs.file-path }}
          destination: 'cicdpipelinee/Trufflehog result'
                      
# Bandit for Source Code Review
  Bandit:
    name: Bandit Scan
    #needs: Trufflehog
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v2

      - name: Set up Python
        uses: actions/setup-python@v2
        with:
          python-version: 3.8

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install bandit
          pip install tabulate
      - name: Security check - Bandit
        run: |
          bandit -r . > bandit_report.txt || true
      - name: Parse Bandit report
        id: parse_output
        run: |
          import re
          import textwrap 
          import json
          import sys
          import os
          import datetime
          from tabulate import tabulate
          # Specify the file path
          file_path = 'bandit_report.txt'
          # To store the table rows
          table_rows = []
          # Open and read the file
          with open(file_path, 'r') as file:
              issue, severity, cwe, location = "", "", "", ""
              collecting_info = False
              for line in file:
                  if 'Issue' in line:
                      issue = textwrap.fill(line.split(': ')[1].strip(), width=40)
                  elif 'Severity' in line:
                      severity = line.split(': ')[1].strip().split(' ')[0]  # Take only severity, exclude 'Confidence'
                  elif 'CWE' in line:
                      cwe = textwrap.fill(line.split(': ')[1].strip(), width=70)  # Capture everything after "CWE:"
                  elif 'Location' in line:
                      location = textwrap.fill(line.split(': ')[1].strip(), width=30)
                      collecting_info = True  # Start collecting info after 'Location'
                  elif '---------------------------' in line:
                      collecting_info = False  # Stop collecting info when encountering the delimiter
                      # Assuming this is the end of the current issue, so we add the row to the list of rows and reset the variables for the next issue.
                      table_rows.append([issue, severity, cwe, location])
                      issue, severity, cwe, location = "", "", "", ""
                  elif collecting_info:
                      # Append all additional lines of information to the "Location" for the current issue
                      location += "\n" + textwrap.fill(line.strip(), width=50)
          # Convert to JSON
          json_data = json.dumps(table_rows)
          # Parse JSON
          parsed_json = json.loads(json_data)
          # Custom sort function
          severity_order = {'High': 0, 'Medium': 1, 'Low': 2}
          # Sort the rows
          sorted_rows = sorted(parsed_json, key=lambda x: severity_order[x[1]])
          # Specify the table headers
          table_headers = ["Issue", "Severity", "CWE", "Location"]
          
          # Print the table
          print(tabulate(sorted_rows, headers=table_headers, tablefmt='grid'))
          # Naming convention for the txt file
          nameofuser = "cairo"  # Replace this with your username
          nameofjob = "bandit"  # Replace this with the job name or any relevant identifier
          # Get the current date and time
          now = datetime.datetime.now()
          date_and_time = now.strftime("%Y-%m-%d_%H-%M")
          filename = f"{nameofuser}_{nameofjob}_{date_and_time}.txt"
          # Save the table to a local file
          file_path = os.path.join(os.path.dirname(os.path.abspath(__file__)), filename)
          with open(file_path, "w") as file:
              file.write(tabulate(sorted_rows, headers=table_headers, tablefmt='grid'))
          print(f'::set-output name=file-path::{file_path}')
        shell: python
        
      - name: Authenticate to Google Cloud
        id: auth
        uses: google-github-actions/auth@v1
        with:
          credentials_json: '${{ secrets.GCS_SA_KEY }}'

      - name: Upload Detect Secrets Output to GCS
        uses: google-github-actions/upload-cloud-storage@v1
        with:
          path: ${{ steps.parse_output.outputs.file-path }}
          destination: 'cicdpipelinee/Bandit result'

      - name: Check for High Severity Issues and Exit
        run: |
          table_file="${{ steps.parse_output.outputs.file-path }}"
          high_severity_found=$(grep -E 'High\s+' "$table_file")
          if [ -n "$high_severity_found" ]; then
            echo "High severity issues found. Exiting workflow..."
            exit 1
          else
            echo "No high severity issues found. Continuing with the workflow."
          fi   
